{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet & WideResnet .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EbF4nR2oXnq9_cp_sADRZFh12DxRF6FY",
      "authorship_tag": "ABX9TyOQ8NUq2hKfME00RvGRWO+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yugonojima/Resnet-WideResnet-model-/blob/main/Resnet_%26_WideResnet_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnet\n",
        "\n",
        "残差ブロック"
      ],
      "metadata": {
        "id": "7_c8LqVhCHC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as kl"
      ],
      "metadata": {
        "id": "3E2Z-NUjG9hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxUYVGjCBo8I"
      },
      "outputs": [],
      "source": [
        "class Res_Block(tf.keras.Model):\n",
        "    def __init__ (self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        bneck_channels = out_channels // 4\n",
        "\n",
        "        self.bn1 = kl.BatchNormalization()\n",
        "        self.av1 = kl.Activation(tf.nn.relu)\n",
        "        self.conv1 = kl.Conv2D(bneck_channels, kernes_size = 1, strides = 1, padding = 'valid', use_bias = False)\n",
        "        \n",
        "        self.bn2 = kl.BatchNormalization()\n",
        "        self.av2 = kl.Activation(tf.nn.relu)\n",
        "        self.conv2 = kl.Conv2D(bneck_channels, kernes_size = 3, strides = 1, padding = 'same', use_bias = False)\n",
        "        \n",
        "        self.bn3 = kl.BatchNormalization()\n",
        "        self.av3 = kl.Activation(tf.nn.relu)\n",
        "        self.conv3 = kl.Conv2D(out_channels, kernes_size = 1, strides = 1, padding = 'valid', use_bias = False)\n",
        "\n",
        "        self.shortcut = self._scblock(in_channels, out_channels)\n",
        "        self.add = kl.Add()\n",
        "\n",
        "    def _scblock(self, in_channels, out_channels):\n",
        "        if in_channels != out_channels:\n",
        "            self.bn_sc1 = kl.BatchNormalization()\n",
        "            self.conv_sc1 = kl.Conv2D(out_channels, kernel_size= 1, strides = 1, padding= 'same', use_bias=False)\n",
        "\n",
        "            return self.conv_sc1\n",
        "        else :\n",
        "            return lambda x : x #xをxのまま返す関数\n",
        "\n",
        "    def call(self, x):\n",
        "         out1 = self.conv1(self.av1(self.bn1(x)))\n",
        "         out2 = self.conv2(self.av2(self.bn2(out1)))        \n",
        "         out3 = self.conv3(self.av3(self.bn3(out2)))\n",
        "         shortcut = self.shortcut(x)    \n",
        "         out4 = self.add([out3, shortcut])\n",
        "\n",
        "         return out4\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wide Resnet"
      ],
      "metadata": {
        "id": "4rGsWnONfSxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import os \n",
        "import os.path as path\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.data_utils import get_file\n",
        "from operator import attrgetter\n"
      ],
      "metadata": {
        "id": "meDRBBGkfWyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from six import b\n",
        "def download_data():\n",
        "    shutil.copytree(get_file('cifar-10-batches-py', origin='http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz', untar=True), './data')\n",
        "\n",
        "def load_data(data_path='./data'):\n",
        "    def load_batch(path):\n",
        "        with open(path, 'rb') as f: #withを利用することで、ファイルを閉じる処理を行わなくてもファイルを閉じる終了処理を実行してくれる\n",
        "                                   #'rb'はバイナリファイルの読み込み\n",
        "            batch = pickle.load(f, encoding='bytes')#load関数: pickle化(バイナリファイルから読み込んだデータはpickle化されている)されたデータを読み込んで非pickle化する\n",
        "\n",
        "        return (np.array(batch[b'data']).reshape(batch[b'data'].shape[0], 3, 32, 32).transpose(0,2,3,1) /255, to_categorical(np.array(batch[b'labels'])))[:1000]\n",
        "\n",
        "    def load_batches(paths):\n",
        "            return tuple(map(np.concatenate, zip(*map(load_batch, paths))))\n",
        "\n",
        "    if not path.exists('./data'):\n",
        "            download_data()\n",
        "\n",
        "    return (load_batches(sorted(map(attrgetter('path'), filter(lambda directory_entry: directory_entry.name.startswith('data_batch_'), os.scandir(data_path))))), \n",
        "            load_batch('{0}/test_batch'.format(data_path)))"
      ],
      "metadata": {
        "id": "GEo2W-lVg0io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install funcy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv_logh00jH1",
        "outputId": "f11377b6-3f60-4ded-c3ad-7692a185e8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Installing collected packages: funcy\n",
            "Successfully installed funcy-1.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from funcy                     import concat, identity, juxt, partial, rcompose, repeat, repeatedly, take\n",
        "from keras.callbacks           import LearningRateScheduler\n",
        "from keras.layers              import Activation, Add, BatchNormalization, Conv2D, Dense, GlobalAveragePooling2D, Input\n",
        "from keras.models              import Model, save_model\n",
        "from tensorflow.keras.optimizers          import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers        import l2\n",
        "from tensorflow.keras.utils               import plot_model\n",
        "from operator                  import getitem\n",
        "\n",
        "#rcompose: 合成関数を簡単に記述することができる関数,前から順に計算される\n",
        "\n",
        "def computational_graph(class_size):\n",
        "\n",
        "    def ljuxt(*fs): #juxt: 同じ引数を持つ二つの関数からの返り値を受け取ることができる関数\n",
        "                   #juxtは返り値がジェネレーターとなるため、ジェネレーターではなく、listで返り値を返せるように定義\n",
        "        return rcompose(juxt(*fs), list)\n",
        "\n",
        "    def batch_normalization():\n",
        "        return BatchNormalization()\n",
        "\n",
        "    def relu():\n",
        "        return Activation('relu')\n",
        "\n",
        "    def conv(filter_size, kernel_size, stride_size=1):\n",
        "        return Conv2D(filter_size, kernel_size, strides=stride_size, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(0.0005), use_bias=False)  # ReLUするならウェイトをHe初期化するのが基本らしい。あと、Kerasにはweight decayがなかったのでkernel_regularizerで代替したのたけど、これで正しい？\n",
        "\n",
        "    def add():\n",
        "        return Add()\n",
        "\n",
        "    def global_average_pooling():\n",
        "        return GlobalAveragePooling2D()\n",
        "\n",
        "    def dense(unit_size, activation):\n",
        "        return Dense(unit_size, activation=activation, kernel_regularizer=l2(0.0005))  \n",
        "\n",
        "    def first_residual_unit(filter_size, stride_size):\n",
        "        return rcompose(batch_normalization(),\n",
        "                        relu(),\n",
        "                        ljuxt(rcompose(conv(filter_size, 3, stride_size),\n",
        "                                       batch_normalization(),\n",
        "                                       relu(),\n",
        "                                       conv(filter_size, 3, 1)),\n",
        "                              rcompose(conv(filter_size, 1, stride_size))),\n",
        "                        add())\n",
        "\n",
        "    def residual_unit(filter_size):\n",
        "        return rcompose(ljuxt(rcompose(batch_normalization(),\n",
        "                                       relu(),\n",
        "                                       conv(filter_size, 3),\n",
        "                                       batch_normalization(),\n",
        "                                       relu(),\n",
        "                                       conv(filter_size, 3)),\n",
        "                              identity),\n",
        "                        add())\n",
        "\n",
        "    def residual_block(filter_size, stride_size, unit_size):\n",
        "        return rcompose(first_residual_unit(filter_size, stride_size),\n",
        "                        rcompose(*repeatedly(partial(residual_unit, filter_size), unit_size - 1)))\n",
        "\n",
        "    k = 10 \n",
        "    n =  4 \n",
        "\n",
        "    return rcompose(conv(16, 3),\n",
        "                    residual_block(16 * k, 1, n),\n",
        "                    residual_block(32 * k, 2, n),\n",
        "                    residual_block(64 * k, 2, n),\n",
        "                    batch_normalization(),\n",
        "                    relu(),\n",
        "                    global_average_pooling(),\n",
        "                    dense(class_size, 'softmax'))\n",
        "\n",
        "\n",
        "def main():\n",
        "    (x_train, y_train), (x_validation, y_validation) = load_data()\n",
        "\n",
        "    model = Model(*juxt(identity, computational_graph(y_train.shape[1]))(Input(shape=x_train.shape[1:])))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=SGD(momentum=0.9), metrics=['accuracy']) \n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    train_data      = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True, width_shift_range=0.125, height_shift_range=0.125, horizontal_flip=True)\n",
        "    validation_data = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "    for data in (train_data, validation_data):\n",
        "        data.fit(x_train) \n",
        "\n",
        "    batch_size = 128\n",
        "    epoch_size = 1\n",
        "\n",
        "    results = model.fit_generator(train_data.flow(x_train, y_train, batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=epoch_size,\n",
        "                                  callbacks=[LearningRateScheduler(partial(getitem, tuple(take(epoch_size, concat(repeat(0.1, 60), repeat(0.02, 60), repeat(0.004, 40), repeat(0.0008))))))],\n",
        "                                  validation_data=validation_data.flow(x_validation, y_validation, batch_size=batch_size),\n",
        "                                  validation_steps=x_validation.shape[0] // batch_size)\n",
        "\n",
        "    # with open('./results/history.pickle', 'wb') as f:\n",
        "    #     pickle.dump(results.history, f)\n",
        "\n",
        "    # save_model(model, './results/model.h5')\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "o4c8I_G5sgt5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}